{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c482fb2",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models (LLMs)\n",
    "\n",
    "Welcome to this introductory notebook on Large Language Models (LLMs)! In this session, we will explore the capabilities of Google's Gemini models, specifically focusing on the multimodal capabilities of gemini-2.0-flash.\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up and configure an LLM API connection\n",
    "- Make basic text-based queries to an LLM\n",
    "- Explore multimodal capabilities (image and audio processing)\n",
    "- Understand the impact of prompting on LLM responses\n",
    "\n",
    "Let's start with a simple \"Hello World\" example to demonstrate the basic functionality of an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from google import genai  # Google's Generative AI library\n",
    "from dotenv import load_dotenv  # For loading environment variables\n",
    "import os\n",
    "\n",
    "# Configuration modules for the LLM\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# This helps keep your API keys secure by storing them in a separate file not checked into version control\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the API key is set in the environment\n",
    "# The API key is required to authenticate with Google's Generative AI services\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Default system prompt that will be used in all our interactions with the model\n",
    "# System prompts help steer the model's behavior and style of responses\n",
    "system_prompt_default = \"You are an assistant that provides concise and accurate answers to user queries. Always say 'Thanks for your question!' at the end of your response.\"\n",
    "\n",
    "def call_llm(prompt: str, system_prompt: str = system_prompt_default) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Google GenAI LLM with the provided prompt and returns the response.\n",
    "    \n",
    "    This function establishes a connection with Google's GenAI API, sends your prompt\n",
    "    to the Gemini model, and returns the generated text response.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt or question to send to the LLM.\n",
    "        system_prompt (str): Instructions that guide the model's behavior.\n",
    "                            Defaults to the system_prompt_default defined above.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text response from the LLM.\n",
    "    \"\"\"\n",
    "    # Initialize the client with your API key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Generate content using the specified model\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",  # Using Gemini 2.0 Flash - a fast, efficient model\n",
    "        contents=prompt,           # Your input prompt/question\n",
    "        \n",
    "        # Configuration parameters to control the generation\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.8,          # Controls randomness: lower = more deterministic outputs\n",
    "            system_instruction=system_prompt,  # Provides overall guidance to the model\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract and return the text from the response\n",
    "    return response.text.strip() if response.text else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic Text Prompt\n",
    "# Let's send a simple factual question to test our LLM connection\n",
    "\n",
    "print(\"Sending prompt: 'What is the capital of France?'\")\n",
    "result = call_llm(\"What is the capital of France?\")\n",
    "print(\"-\" * 50)  # Separator for better readability\n",
    "print(f\"LLM Response:\\n{result}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Note: The model's response should include the fact that Paris is the capital of France,\n",
    "# and end with \"Thanks for your question!\" as instructed in our system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccceddc0",
   "metadata": {},
   "source": [
    "### gemini-flash-2.0 is multimodal\n",
    "\n",
    "## Exploring Multimodal Capabilities\n",
    "\n",
    "One of the most powerful features of modern LLMs like Gemini is their ability to understand and process multiple modalities of information - not just text, but also images, audio, and more. This allows for much more versatile and useful applications.\n",
    "\n",
    "### Image Understanding\n",
    "\n",
    "Let's explore how the model can analyze and describe images. Multimodal models can:\n",
    "- Identify objects and people in images\n",
    "- Describe scenes and settings\n",
    "- Understand visual contexts\n",
    "- Extract text from images\n",
    "- Answer questions about visual content\n",
    "\n",
    "Let's try with this image:\n",
    "<br>\n",
    "![alt text](sample_image_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask about the image\n",
    "\n",
    "def call_llm_image(prompt: str, image_path: str, system_prompt: str = system_prompt_default) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Google GenAI LLM with both text prompt and image input, returning the response.\n",
    "    \n",
    "    This demonstrates how Gemini can process and understand images alongside text,\n",
    "    enabling multimodal interactions.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The text prompt or question about the image.\n",
    "        image_path (str): The path to the image file to analyze.\n",
    "        system_prompt (str): Instructions for the model's behavior.\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response about the image.\n",
    "    \"\"\"\n",
    "    # Initialize the client with your API key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Upload the image file to the API\n",
    "    uploaded_image = client.files.upload(\n",
    "        file=image_path,\n",
    "    )\n",
    "    \n",
    "    # Generate content using both the text prompt and image\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",  # Using Gemini 2.0 Flash \n",
    "        contents=[prompt, uploaded_image],  # Both text and image inputs\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.2,\n",
    "            system_instruction=system_prompt,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract and return the text from the response\n",
    "    return response.text.strip() if response.text else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Image Analysis\n",
    "# Let's ask the model to describe what it sees in our first image\n",
    "\n",
    "print(\"Analyzing image: sample_image_1.png\")\n",
    "print(\"Prompt: 'What is in this image?'\")\n",
    "\n",
    "image_result = call_llm_image(\n",
    "    \"What is in this image?\",\n",
    "    \"sample_image_1.png\"\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM Image Response:\\n{image_result}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Note: The model will analyze the visual content and provide a description\n",
    "# of what it sees in the image, identifying objects, scenery, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04d02e",
   "metadata": {},
   "source": [
    "### Testing with a More Complex Image\n",
    "\n",
    "Let's challenge the model with a more complex image. Complex images can include:\n",
    "- Multiple objects or subjects\n",
    "- Detailed backgrounds\n",
    "- Text within the image\n",
    "- Abstract concepts or scenes\n",
    "- Unusual perspectives\n",
    "\n",
    "This allows us to better understand the model's visual processing capabilities and limitations.\n",
    "\n",
    "<br>\n",
    "<img src=\"sample_image_2.jpg\" alt=\"alt text\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Complex Image Analysis\n",
    "# Testing the model's ability to handle more detailed or complex images\n",
    "\n",
    "print(\"Analyzing image: sample_image_2.jpg\")\n",
    "print(\"Prompt: 'What is in this image?'\")\n",
    "\n",
    "image_result = call_llm_image(\n",
    "    \"What is in this image?\",\n",
    "    \"sample_image_2.jpg\"\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM Image Response:\\n{image_result}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Examining how well the model can identify and describe elements in a more complex image\n",
    "# We're using the same prompt as before to compare how the model handles different levels of visual complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcffdf",
   "metadata": {},
   "source": [
    "We tried with images, now we'll try with an audio snippet\n",
    "\n",
    "## Audio Processing Capabilities\n",
    "\n",
    "Another impressive capability of multimodal LLMs is their ability to process and understand audio data.\n",
    "This opens up possibilities for:\n",
    "\n",
    "- Speech transcription\n",
    "- Audio content analysis\n",
    "- Voice command processing\n",
    "- Language translation from spoken content\n",
    "- Content summarization from audio\n",
    "\n",
    "Let's see how Gemini can process an audio snippet and extract the spoken information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ff0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to handle audio-based prompting\n",
    "\n",
    "def call_llm_audio(prompt: str, audio_path: str, system_prompt: str = system_prompt_default) -> str:\n",
    "    \"\"\"\n",
    "    Calls the Google GenAI LLM with both text prompt and audio input, returning the response.\n",
    "    \n",
    "    This demonstrates how Gemini can process and understand audio alongside text,\n",
    "    enabling another dimension of multimodal interactions.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The text prompt or instruction about the audio.\n",
    "        audio_path (str): The path to the audio file to process.\n",
    "        system_prompt (str): Instructions for the model's behavior.\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response about the audio content.\n",
    "    \"\"\"\n",
    "    # Initialize the client with your API key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Upload the audio file to the API\n",
    "    uploaded_audio = client.files.upload(\n",
    "        file=audio_path,\n",
    "    )\n",
    "    \n",
    "    # Generate content using both the text prompt and audio\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[prompt, uploaded_audio],  # Both text and audio inputs\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.2,\n",
    "            system_instruction=system_prompt,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Extract and return the text from the response\n",
    "    return response.text.strip() if response.text else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Basic Audio Processing\n",
    "# First attempt at audio transcription with a general prompt\n",
    "\n",
    "print(\"Processing audio: sample_audio.mp3\")\n",
    "print(\"Prompt: 'What is being said in this audio?'\")\n",
    "\n",
    "audio_result = call_llm_audio(\n",
    "    \"What is being said in this audio?\",\n",
    "    \"sample_audio.mp3\"\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM Audio Response:\\n{audio_result}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# This example demonstrates how the model can listen to audio content\n",
    "# and extract the spoken information using a general prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Improved Audio Processing with Better Prompting\n",
    "# Demonstrating how prompt engineering can affect results\n",
    "\n",
    "print(\"Processing audio: sample_audio.mp3\")\n",
    "print(\"Improved prompt: 'Please transcribe the audio'\")\n",
    "\n",
    "audio_result_better_prompt = call_llm_audio(\n",
    "    \"Please transcribe the audio\",\n",
    "    \"sample_audio.mp3\"\n",
    ")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM Audio Response with better prompt:\\n{audio_result_better_prompt}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# This example demonstrates an important concept in working with LLMs:\n",
    "# The quality and specificity of your prompt can significantly affect the results.\n",
    "# By asking specifically for a transcription rather than a general description,\n",
    "# we may get more accurate or detailed text from the audio content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60c05a",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've explored the fundamental capabilities of Large Language Models, specifically Google's Gemini 2.0 Flash model:\n",
    "\n",
    "1. **Text Processing**: We started with basic text queries, showing how the model can answer factual questions.\n",
    "\n",
    "2. **Multimodal Capabilities**:\n",
    "   - **Image Analysis**: We demonstrated how the model can interpret and describe both simple and complex images.\n",
    "   - **Audio Processing**: We showed how the model can transcribe and interpret audio content.\n",
    "\n",
    "3. **Prompt Engineering**: We demonstrated how the quality and specificity of prompts can significantly affect the results you get from an LLM.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Modern LLMs like Gemini can process multiple types of data (text, images, audio) in a single interaction\n",
    "- The way you phrase your prompts has a meaningful impact on the quality of responses\n",
    "- System prompts can help guide the model's behavior and response style\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To continue learning about LLMs, you might want to explore:\n",
    "- More advanced prompt engineering techniques\n",
    "- Fine-tuning models for specific use cases\n",
    "- Implementing LLMs in real-world applications\n",
    "- Exploring limitations and ethical considerations of LLM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c9cf6",
   "metadata": {},
   "source": [
    "### Additional Resources and References \n",
    "\n",
    "I've based this notebook mostly from this youtube video\n",
    "- [Gemini API Introduction](https://youtu.be/qfWpPEgea2A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tip-llm-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
