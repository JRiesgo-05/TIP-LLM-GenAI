{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb98a2d2",
   "metadata": {},
   "source": [
    "# RAG with PDF using Gemini SDK\n",
    "\n",
    "This notebook demonstrates how to implement a Retrieval-Augmented Generation (RAG) system with PDF documents using Google's Gemini SDK. We'll go through the following steps:\n",
    "\n",
    "1. Import and setup the necessary libraries\n",
    "2. Load and process a PDF document\n",
    "3. Generate text embeddings\n",
    "4. Create a vector store for efficient retrieval\n",
    "5. Implement the RAG pipeline with Gemini\n",
    "6. Query the system with questions about the PDF document\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f6175",
   "metadata": {},
   "source": [
    "## 1. Import and Setup Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries:\n",
    "- `google.generativeai`: The Gemini SDK for accessing Google's generative AI models\n",
    "- `PyPDF2`: For parsing PDF files\n",
    "- `langchain`: Framework for working with language models and building RAG applications\n",
    "- `faiss`: For vector storage and similarity search\n",
    "- Other utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from google import genai\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from google.genai import types\n",
    "\n",
    "# For vector storage and retrieval\n",
    "import faiss\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For env vars (API keys)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Set up Google API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Please set your GOOGLE_API_KEY in .env file or environment variables\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d7455",
   "metadata": {},
   "source": [
    "## 2. Load and Process PDF Document\n",
    "\n",
    "Now we'll load and process the PDF file `sample_pdf.pdf`. We'll extract all the text from the document and then split it into smaller chunks for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text content from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "\n",
    "# Define the path to the PDF file\n",
    "pdf_path = \"../notebooks/sample_pdf.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Display the first 500 characters of the extracted text\n",
    "print(f\"PDF Length: {len(raw_text)} characters\")\n",
    "print(\"\\nPreview of the extracted text:\")\n",
    "print(raw_text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into smaller chunks for processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "text_chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(f\"Split the document into {len(text_chunks)} chunks\")\n",
    "print(f\"\\nSample chunk (first chunk):\\n{text_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70d260",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings\n",
    "\n",
    "We'll use Google's embedding model to convert our text chunks into vector embeddings. These embeddings capture the semantic meaning of the text and allow us to perform similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding(text, model=\"text-embedding-004\", task_type=\"RETRIEVAL_DOCUMENT\", title=None):\n",
    "    \"\"\"\n",
    "    Get embedding vector for text using Google's Embedding API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to embed\n",
    "        model (str): The model to use\n",
    "        task_type (str): The task type for the embedding\n",
    "        title (str, optional): Optional title for the document\n",
    "        \n",
    "    Returns:\n",
    "        list: The embedding vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.genai import types\n",
    "        config = types.EmbedContentConfig(\n",
    "            task_type=task_type,\n",
    "            title=title\n",
    "        )\n",
    "        \n",
    "        result = client.models.embed_content(\n",
    "            model=model,\n",
    "            contents=text,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        return result.embeddings[0].values if result.embeddings else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generate embeddings for all text chunks\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Generating embedding for chunk {i+1}/{len(text_chunks)} ({(i+1)/len(text_chunks)*100:.1f}%)...\", end=\"\\r\")\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "print(f\"\\nGenerated {len(embeddings)} embeddings\")\n",
    "\n",
    "# Convert to numpy array for further processing\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "print(f\"Embedding dimensions: {embeddings_array.shape}\")  # Should be (num_chunks, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59442396",
   "metadata": {},
   "source": [
    "## 4. Create Vector Store\n",
    "\n",
    "Now we'll create a vector store using FAISS, a library for efficient similarity search. This will allow us to quickly find the most similar text chunks to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08200897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS index for vector search\n",
    "embedding_dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dimension)  # L2 distance for similarity search\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(embeddings_array)\n",
    "print(f\"Added {index.ntotal} vectors to the FAISS index\")\n",
    "\n",
    "def search_similar_chunks(query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search for chunks similar to the query and return them with their similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with 'chunk', 'score', and 'id' keys\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    query_embedding_array = np.array([query_embedding]).astype('float32')\n",
    "    \n",
    "    # Search in the index\n",
    "    distances, indices = index.search(query_embedding_array, top_k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx != -1:  # Valid result\n",
    "            results.append({\n",
    "                'chunk': text_chunks[idx],\n",
    "                'score': float(1 / (1 + distances[0][i])),  # Convert distance to similarity score\n",
    "                'id': int(idx)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the search function\n",
    "test_query = \"What is this document about?\"\n",
    "similar_chunks = search_similar_chunks(test_query, top_k=2)\n",
    "\n",
    "print(\"Test search results:\")\n",
    "for i, result in enumerate(similar_chunks):\n",
    "    print(f\"\\nResult {i+1} (Score: {result['score']:.4f}):\")\n",
    "    print(f\"Chunk {result['id']}: {result['chunk'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8f91d",
   "metadata": {},
   "source": [
    "## 5. Implement RAG Pipeline\n",
    "\n",
    "Now we'll implement the full RAG pipeline, which involves:\n",
    "1. Taking a user query\n",
    "2. Finding relevant text chunks from our document\n",
    "3. Using these chunks as context for Gemini to generate an accurate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dadf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_rag_response(query: str, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response to a query using RAG with the Gemini model.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        top_k: Number of similar chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Generated response from the model\n",
    "    \"\"\"\n",
    "    # 1. Retrieve similar chunks\n",
    "    similar_chunks = search_similar_chunks(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([result[\"chunk\"] for result in similar_chunks])\n",
    "    \n",
    "    # 2. Create prompt with context\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant that answers questions based on provided context information. \n",
    "    Answer the following question using ONLY the context provided below. \n",
    "    If you can't answer the question based on the context, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    QUESTION:\n",
    "    {query}\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Generate response with Gemini\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",  # Using Gemini 2.0 Flash - a fast, efficient model\n",
    "        contents=prompt,           # Your input prompt/question\n",
    "        # Configuration parameters to control the generation\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.8,          # Controls randomness: lower = more deterministic outputs\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Test the RAG pipeline\n",
    "test_question = \"What is the main topic of this document?\"\n",
    "rag_answer = generate_rag_response(test_question)\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nAnswer: {rag_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604c2f7",
   "metadata": {},
   "source": [
    "## 6. Query the System\n",
    "\n",
    "Now that our RAG system is set up, let's ask it some questions about the PDF document. You can try different questions to see how the system responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for interactive questioning\n",
    "def ask_document(question: str) -> None:\n",
    "    \"\"\"\n",
    "    Ask a question about the document and get a response using our RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ” Searching for relevant context...\")\n",
    "    answer = generate_rag_response(question, top_k=3)\n",
    "    print(f\"\\nðŸ¤– Answer: {answer}\")\n",
    "\n",
    "# Try some sample questions\n",
    "questions = [\n",
    "    \"What is the main topic of this document?\",\n",
    "    \"What are the key points made in this document?\",\n",
    "    \"Can you summarize the document for me?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n\\n>>> Question: {question}\")\n",
    "    ask_document(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb4c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cell for user questions\n",
    "user_question = \"What is the significance of this document?\" # Change this to your question\n",
    "\n",
    "ask_document(user_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd5046",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to implement a simple RAG system with PDF documents using the Gemini SDK:\n",
    "\n",
    "1. We loaded and extracted text from a PDF document\n",
    "2. We split the text into manageable chunks\n",
    "3. We generated embeddings for each chunk using Google's embedding model\n",
    "4. We created a FAISS index for efficient similarity search\n",
    "5. We set up a RAG pipeline that retrieves relevant context and generates answers with Gemini\n",
    "6. We tested the system with various questions about the document\n",
    "\n",
    "This approach allows for more accurate and grounded responses from the LLM by providing it with relevant context from our document. The system can be extended to handle multiple documents, different file formats, and more sophisticated retrieval methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tip-llm-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
