{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052ef8e5",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this notebook, we'll explore Retrieval Augmented Generation (RAG), a powerful technique that enhances Large Language Models (LLMs) by enabling them to access and use external knowledge beyond their training data.\n",
    "\n",
    "We'll cover the following topics:\n",
    "1. Introduction to Embeddings and Vector Representations\n",
    "2. Using Google's Embeddings API\n",
    "3. Building a Simple RAG System\n",
    "4. Evaluating and Fine-tuning RAG Performance\n",
    "5. Advanced RAG Techniques\n",
    "\n",
    "Let's start by importing the necessary libraries and exploring the concept of embeddings, which are the foundation of effective RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "import scipy.spatial.distance as distance\n",
    "\n",
    "# Load environment variables from .env file (if you have any API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the Google Generative AI API with your API key\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6923f8f",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's create some utility functions to make it easier to interact with the language model and embeddings throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfac2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gemini_response(prompt, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Get a response from Google's Gemini model\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the model\n",
    "        model (str): The model to use (default: gemini-2.0-flash)\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the model\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40,\n",
    "            \"max_output_tokens\": 1024,\n",
    "        }\n",
    "        \n",
    "        # Generate the response\n",
    "        response = client.models.generate_content(\n",
    "            contents=prompt,\n",
    "            model=model,\n",
    "            config=generation_config,\n",
    "        )\n",
    "        \n",
    "        # Return the response text\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-004\", task_type=\"RETRIEVAL_DOCUMENT\", title=None):\n",
    "    \"\"\"\n",
    "    Get embedding vector for text using Google's Embedding API\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to embed\n",
    "        model (str): The model to use\n",
    "        task_type (str): The task type for the embedding\n",
    "        title (str, optional): Optional title for the document\n",
    "        \n",
    "    Returns:\n",
    "        list: The embedding vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.genai import types\n",
    "        config = types.EmbedContentConfig(\n",
    "            task_type=task_type,\n",
    "            title=title\n",
    "        )\n",
    "        \n",
    "        result = client.models.embed_content(\n",
    "            model=model,\n",
    "            contents=text,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        return result.embeddings[0].values if result.embeddings else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors\n",
    "    \n",
    "    Args:\n",
    "        v1 (list): First vector\n",
    "        v2 (list): Second vector\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity (-1 to 1)\n",
    "    \"\"\"\n",
    "    return 1 - distance.cosine(v1, v2)\n",
    "\n",
    "def display_prompt_response(prompt, response, technique=\"\"):\n",
    "    \"\"\"\n",
    "    Display the prompt and response in a formatted way\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The prompt sent to the model\n",
    "        response (str): The model's response\n",
    "        technique (str): The technique used\n",
    "    \"\"\"\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"TECHNIQUE: {technique}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\\nüìù PROMPT:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(prompt)\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(\"\\nü§ñ RESPONSE:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(response)\n",
    "    print(f\"{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e6cef",
   "metadata": {},
   "source": [
    "## 1. Introduction to Embeddings and Vector Representations\n",
    "\n",
    "Embeddings are numerical representations of data (like text, images, or audio) in a high-dimensional vector space. They capture semantic meaning in a way that similar concepts are positioned closer to each other in this space.\n",
    "\n",
    "### How Embeddings Work\n",
    "\n",
    "When we convert words, sentences, or documents into embeddings, we're essentially mapping them to points in a multi-dimensional space. The position of these points captures the semantic relationships between the words or concepts they represent.\n",
    "\n",
    "Let's visualize this with a simple example comparing words like \"king\", \"queen\", and \"baseball\" to see how embeddings capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d60f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare word embeddings to see how they capture semantic relationships\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"baseball\", \"sports\", \"computer\", \"technology\"]\n",
    "\n",
    "# Get embeddings for these words\n",
    "word_embeddings = {}\n",
    "for word in words:\n",
    "    embedding = get_embedding(word, task_type=\"SEMANTIC_SIMILARITY\")\n",
    "    word_embeddings[word] = embedding\n",
    "\n",
    "# Calculate similarities between pairs of words\n",
    "print(\"Word Similarity Comparisons (Cosine Similarity):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparisons = [\n",
    "    (\"king\", \"queen\"),\n",
    "    (\"king\", \"baseball\"),\n",
    "    (\"man\", \"woman\"),\n",
    "    (\"queen\", \"woman\"),\n",
    "    (\"baseball\", \"sports\"),\n",
    "    (\"computer\", \"technology\"),\n",
    "    (\"king\", \"man\"),\n",
    "    (\"queen\", \"technology\")\n",
    "]\n",
    "\n",
    "for word1, word2 in comparisons:\n",
    "    similarity = cosine_similarity(word_embeddings[word1], word_embeddings[word2])\n",
    "    print(f\"{word1} vs {word2}: {similarity:.4f}\")\n",
    "\n",
    "# Let's visualize the relationships on a 2D plot\n",
    "# We'll use PCA to reduce dimensions for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extract embeddings and corresponding words\n",
    "word_list = list(word_embeddings.keys())\n",
    "embedding_list = [word_embeddings[word] for word in word_list]\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embedding_list)\n",
    "\n",
    "# Plot the words in 2D space\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.7)\n",
    "\n",
    "# Label each point with its word\n",
    "for i, word in enumerate(word_list):\n",
    "    plt.annotate(word, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), \n",
    "                 fontsize=12, alpha=0.8)\n",
    "\n",
    "plt.title(\"Word Embeddings Visualized in 2D Space\", fontsize=14)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05706d4",
   "metadata": {},
   "source": [
    "## 2. Using Google's Embeddings API\n",
    "\n",
    "Google provides powerful embedding models through its Generative AI API. The `text-embedding-004` model can generate high-quality embeddings for various use cases including:\n",
    "\n",
    "- Semantic search\n",
    "- Document retrieval\n",
    "- Question answering\n",
    "- Recommendation systems\n",
    "- Text classification\n",
    "\n",
    "The API allows us to create embeddings optimized for different tasks using the `task_type` parameter:\n",
    "- `RETRIEVAL_QUERY`: For search queries\n",
    "- `RETRIEVAL_DOCUMENT`: For documents to be searched\n",
    "- `SEMANTIC_SIMILARITY`: For semantic textual similarity tasks\n",
    "- `CLASSIFICATION`: For text classification tasks\n",
    "- `CLUSTERING`: For clustering tasks\n",
    "\n",
    "Let's explore how to use these embeddings for different purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4901a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Creating embeddings for different texts\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A fast auburn canine leaps above the idle hound.\",\n",
    "    \"Machine learning models require large amounts of training data.\",\n",
    "    \"Artificial intelligence systems need extensive datasets for training.\"\n",
    "]\n",
    "\n",
    "# Get embeddings for these texts\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    embedding = get_embedding(text, task_type=\"SEMANTIC_SIMILARITY\")\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = np.zeros((len(texts), len(texts)))\n",
    "for i in range(len(texts)):\n",
    "    for j in range(len(texts)):\n",
    "        similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "\n",
    "# Display the similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='viridis')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title(\"Semantic Text Similarity Matrix\")\n",
    "plt.xticks(np.arange(len(texts)), [f\"Text {i+1}\" for i in range(len(texts))], rotation=45)\n",
    "plt.yticks(np.arange(len(texts)), [f\"Text {i+1}\" for i in range(len(texts))])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(texts)):\n",
    "    for j in range(len(texts)):\n",
    "        plt.text(j, i, f\"{similarity_matrix[i, j]:.2f}\", \n",
    "                 ha=\"center\", va=\"center\", color=\"white\" if similarity_matrix[i, j] < 0.7 else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual texts for reference\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "\n",
    "# Example 2: Task-specific embeddings\n",
    "query = \"How does artificial intelligence work?\"\n",
    "document1 = \"Artificial intelligence works by training machine learning models on large datasets to recognize patterns and make predictions.\"\n",
    "document2 = \"The history of artificial intelligence dates back to the 1950s when early computer scientists began exploring the concept.\"\n",
    "\n",
    "# Get task-specific embeddings\n",
    "query_embedding = get_embedding(query, task_type=\"RETRIEVAL_QUERY\")\n",
    "doc1_embedding = get_embedding(document1, task_type=\"RETRIEVAL_DOCUMENT\", title=\"AI Functionality\")\n",
    "doc2_embedding = get_embedding(document2, task_type=\"RETRIEVAL_DOCUMENT\", title=\"AI History\")\n",
    "\n",
    "# Calculate relevance scores\n",
    "relevance1 = cosine_similarity(query_embedding, doc1_embedding)\n",
    "relevance2 = cosine_similarity(query_embedding, doc2_embedding)\n",
    "\n",
    "print(\"\\nQuery relevance scores:\")\n",
    "print(f\"Document 1 (about AI functionality): {relevance1:.4f}\")\n",
    "print(f\"Document 2 (about AI history): {relevance2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e77be1",
   "metadata": {},
   "source": [
    "## 3. Building a Simple RAG System\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique that combines the power of large language models with the ability to access external knowledge. RAG systems typically follow these steps:\n",
    "\n",
    "1. **Document Chunking**: Break documents into manageable chunks\n",
    "2. **Embedding Generation**: Convert document chunks into embeddings\n",
    "3. **Storage**: Store the embeddings and their corresponding text chunks\n",
    "4. **Retrieval**: When a query arrives, convert it to an embedding and find relevant documents\n",
    "5. **Augmented Generation**: Feed the retrieved information to the LLM along with the query to generate a response\n",
    "\n",
    "Let's build a simple RAG system from scratch using Google's Gemini API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e41acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a document collection\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"What is RAG?\",\n",
    "        \"content\": \"\"\"\n",
    "        Retrieval Augmented Generation (RAG) is a technique that enhances large language models (LLMs) \n",
    "        by providing them with relevant information retrieved from an external knowledge base. \n",
    "        This allows the LLM to generate more accurate, up-to-date, and contextually relevant responses.\n",
    "        \n",
    "        RAG systems combine the broad knowledge of LLMs with specialized, current, or proprietary information, \n",
    "        making them powerful tools for knowledge-intensive tasks. They are particularly useful when dealing with \n",
    "        domain-specific questions, recent events, or proprietary data that wasn't part of the LLM's training.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"How RAG Works\",\n",
    "        \"content\": \"\"\"\n",
    "        RAG systems typically work in two phases: retrieval and generation.\n",
    "        \n",
    "        In the retrieval phase, when a user asks a question, the system:\n",
    "        1. Converts the query into an embedding (a numerical representation)\n",
    "        2. Searches a knowledge base for relevant documents by comparing embeddings\n",
    "        3. Retrieves the most semantically similar documents\n",
    "        \n",
    "        In the generation phase, the system:\n",
    "        1. Combines the original query with the retrieved information\n",
    "        2. Sends this augmented context to the LLM\n",
    "        3. The LLM generates a response based on both its pre-trained knowledge and the retrieved information\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Benefits of RAG\",\n",
    "        \"content\": \"\"\"\n",
    "        RAG offers several advantages over using vanilla LLMs:\n",
    "        \n",
    "        1. Reduced hallucinations: By grounding responses in retrieved facts, RAG reduces the tendency of LLMs to generate plausible-sounding but incorrect information.\n",
    "        \n",
    "        2. Access to current information: RAG can incorporate recent information that wasn't available during the LLM's training.\n",
    "        \n",
    "        3. Domain specialization: Organizations can add specific domain knowledge without fine-tuning the entire model.\n",
    "        \n",
    "        4. Transparency and attribution: RAG systems can cite the sources of information used to generate responses.\n",
    "        \n",
    "        5. Privacy and data control: Sensitive information can be kept in private knowledge bases rather than being included in model training.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Challenges in RAG Implementation\",\n",
    "        \"content\": \"\"\"\n",
    "        Implementing effective RAG systems comes with several challenges:\n",
    "        \n",
    "        1. Information retrieval quality: The system's effectiveness depends greatly on retrieving the right information. Poor retrieval leads to irrelevant context and potentially incorrect answers.\n",
    "        \n",
    "        2. Context window limitations: LLMs have finite context windows, limiting how much retrieved information can be included.\n",
    "        \n",
    "        3. Balancing relevance: Determining how much weight to give to retrieved information versus the model's pre-trained knowledge can be tricky.\n",
    "        \n",
    "        4. Knowledge base management: Creating, updating, and maintaining the knowledge base requires ongoing effort.\n",
    "        \n",
    "        5. Computational overhead: RAG systems involve additional processing steps compared to simple LLM inference.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Advanced RAG Techniques\",\n",
    "        \"content\": \"\"\"\n",
    "        Recent advancements in RAG include:\n",
    "        \n",
    "        1. Hybrid search: Combining keyword-based and semantic search for better retrieval.\n",
    "        \n",
    "        2. Re-ranking: Using a separate model to re-rank initially retrieved documents for better relevance.\n",
    "        \n",
    "        3. Query rewriting: Transforming the user's query to better match relevant documents.\n",
    "        \n",
    "        4. Multi-vector retrieval: Representing documents with multiple embeddings to capture different aspects.\n",
    "        \n",
    "        5. Self-RAG: Systems that can decide when to retrieve information and perform retrieval iteratively.\n",
    "        \n",
    "        6. Adaptive retrieval: Dynamically adjusting how much and what kind of information to retrieve based on the query.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Step 2: Define a function to chunk documents\n",
    "def chunk_document(doc, chunk_size=250, overlap=50):\n",
    "    \"\"\"\n",
    "    Split a document into smaller chunks with overlap\n",
    "    \n",
    "    Args:\n",
    "        doc (dict): Document with title and content\n",
    "        chunk_size (int): Approximate size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        list: List of document chunks with title and content\n",
    "    \"\"\"\n",
    "    content = doc[\"content\"]\n",
    "    title = doc[\"title\"]\n",
    "    \n",
    "    # Simple chunking by characters (in a real application, you'd chunk by sentences or paragraphs)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(content):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Adjust end to not cut words\n",
    "        if end < len(content):\n",
    "            # Try to find a space to break at\n",
    "            space_pos = content.rfind(' ', start, end)\n",
    "            if space_pos != -1:\n",
    "                end = space_pos\n",
    "        \n",
    "        # Create the chunk\n",
    "        chunk_content = content[start:end].strip()\n",
    "        if chunk_content:\n",
    "            chunks.append({\n",
    "                \"title\": title,\n",
    "                \"content\": chunk_content\n",
    "            })\n",
    "        \n",
    "        # Move start position for next chunk, including overlap\n",
    "        start = end - overlap if end - overlap > start else end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Step 3: Chunk all documents\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = chunk_document(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# Preview the chunks\n",
    "print(f\"Created {len(all_chunks)} chunks from {len(documents)} documents\")\n",
    "print(\"\\nFirst chunk example:\")\n",
    "print(f\"Title: {all_chunks[0]['title']}\")\n",
    "print(f\"Content: {all_chunks[0]['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create embeddings for the chunks\n",
    "chunk_embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(all_chunks)}...\", end=\"\\r\")\n",
    "    \n",
    "    # Create a combined text with title for better context\n",
    "    text_to_embed = f\"{chunk['title']}: {chunk['content']}\"\n",
    "    \n",
    "    # Get the embedding\n",
    "    embedding = get_embedding(\n",
    "        text_to_embed, \n",
    "        task_type=\"RETRIEVAL_DOCUMENT\", \n",
    "        title=chunk['title']\n",
    "    )\n",
    "    \n",
    "    # Store the chunk and its embedding\n",
    "    if embedding:\n",
    "        chunk_embeddings.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(chunk_embeddings)} embeddings successfully\")\n",
    "\n",
    "# Step 5: Create a simple vector store class\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self, items=None):\n",
    "        \"\"\"Initialize the vector store\"\"\"\n",
    "        self.items = items or []\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        \"\"\"Add a single item to the store\"\"\"\n",
    "        self.items.append(item)\n",
    "    \n",
    "    def add_items(self, items):\n",
    "        \"\"\"Add multiple items to the store\"\"\"\n",
    "        self.items.extend(items)\n",
    "    \n",
    "    def search(self, query_embedding, top_k=3):\n",
    "        \"\"\"Search for similar items based on embedding similarity\"\"\"\n",
    "        if not self.items:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [\n",
    "            (item, cosine_similarity(query_embedding, item[\"embedding\"]))\n",
    "            for item in self.items\n",
    "        ]\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        return similarities[:top_k]\n",
    "\n",
    "# Create our vector store\n",
    "vector_store = SimpleVectorStore()\n",
    "vector_store.add_items(chunk_embeddings)\n",
    "\n",
    "print(f\"Vector store created with {len(vector_store.items)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a RAG query function\n",
    "def rag_query(user_query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform a RAG query\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): User's question\n",
    "        vector_store: Vector store containing document chunks and embeddings\n",
    "        top_k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # 1. Convert the query to an embedding\n",
    "    query_embedding = get_embedding(user_query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return \"Error: Could not generate embedding for query.\"\n",
    "    \n",
    "    # 2. Retrieve relevant documents\n",
    "    search_results = vector_store.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # 3. Format the retrieved context\n",
    "    context_chunks = []\n",
    "    for item, score in search_results:\n",
    "        context_chunks.append(f\"Title: {item['chunk']['title']}\\nContent: {item['chunk']['content']}\\nRelevance: {score:.4f}\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # 4. Create the augmented prompt\n",
    "    prompt = f\"\"\"\n",
    "    Answer the following question based on the provided context. If the context doesn't contain relevant information, \n",
    "    say that you don't have enough information to answer accurately.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 5. Generate response using the LLM\n",
    "    response = get_gemini_response(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Let's test our RAG system with some queries\n",
    "test_queries = [\n",
    "    \"What is Retrieval Augmented Generation?\",\n",
    "    \"What are the main benefits of using RAG?\",\n",
    "    \"How can I improve the retrieval quality in a RAG system?\",\n",
    "    \"What is quantum computing?\" # A query our knowledge base doesn't cover\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nQuery {i+1}: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = rag_query(query, vector_store, top_k=2)\n",
    "    print(response)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f046bfd",
   "metadata": {},
   "source": [
    "## 4. Evaluating and Fine-tuning RAG Performance\n",
    "\n",
    "For RAG systems to be effective, both the retrieval and generation components need to work well together. Let's look at some ways to evaluate and improve RAG performance:\n",
    "\n",
    "### Key Metrics for Evaluation\n",
    "\n",
    "1. **Retrieval Performance**:\n",
    "   - Precision: Are the retrieved documents relevant?\n",
    "   - Recall: Did we retrieve all the relevant documents?\n",
    "   - Mean Reciprocal Rank (MRR): How high were relevant documents ranked?\n",
    "\n",
    "2. **Generation Quality**:\n",
    "   - Factual accuracy: Does the answer contain correct information?\n",
    "   - Relevance: Does the answer address the question?\n",
    "   - Coherence: Is the answer well-structured and understandable?\n",
    "   - Citation accuracy: Does the model properly use the provided context?\n",
    "\n",
    "Let's implement some simple evaluation techniques for our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple evaluation framework for our RAG system\n",
    "\n",
    "# Define some test cases with ground truth\n",
    "evaluation_cases = [\n",
    "    {\n",
    "        \"query\": \"What is RAG and how does it work?\",\n",
    "        \"relevant_docs\": [\"What is RAG?\", \"How RAG Works\"],  # Titles of relevant documents\n",
    "        \"key_points\": [\n",
    "            \"combines retrieval and generation\",\n",
    "            \"enhances LLMs with external knowledge\",\n",
    "            \"two phases: retrieval and generation\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the main benefits of using RAG systems?\",\n",
    "        \"relevant_docs\": [\"Benefits of RAG\"],\n",
    "        \"key_points\": [\n",
    "            \"reduced hallucinations\",\n",
    "            \"access to current information\",\n",
    "            \"domain specialization\",\n",
    "            \"transparency\",\n",
    "            \"privacy\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What challenges might I face when implementing RAG?\",\n",
    "        \"relevant_docs\": [\"Challenges in RAG Implementation\"],\n",
    "        \"key_points\": [\n",
    "            \"information retrieval quality\",\n",
    "            \"context window limitations\",\n",
    "            \"knowledge base management\",\n",
    "            \"computational overhead\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to evaluate retrieval performance\n",
    "def evaluate_retrieval(query, relevant_doc_titles, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval component of RAG\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "        relevant_doc_titles (list): Titles of relevant documents\n",
    "        vector_store: The vector store\n",
    "        top_k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Convert the query to an embedding\n",
    "    query_embedding = get_embedding(query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return {\"error\": \"Could not generate embedding\"}\n",
    "    \n",
    "    # Retrieve documents\n",
    "    results = vector_store.search(query_embedding, top_k=top_k)\n",
    "    retrieved_titles = [item[\"chunk\"][\"title\"] for item, _ in results]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    relevant_retrieved = [title for title in retrieved_titles if title in relevant_doc_titles]\n",
    "    \n",
    "    precision = len(relevant_retrieved) / len(retrieved_titles) if retrieved_titles else 0\n",
    "    recall = len(relevant_retrieved) / len(relevant_doc_titles) if relevant_doc_titles else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Calculate MRR (Mean Reciprocal Rank)\n",
    "    mrr = 0\n",
    "    for i, title in enumerate(retrieved_titles):\n",
    "        if title in relevant_doc_titles:\n",
    "            mrr = 1 / (i + 1)  # Rank is 1-indexed\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mrr\": mrr,\n",
    "        \"retrieved_titles\": retrieved_titles\n",
    "    }\n",
    "\n",
    "# Function to evaluate the generated answer\n",
    "def evaluate_generation(query, key_points, response):\n",
    "    \"\"\"\n",
    "    Evaluate the generation component of RAG\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query\n",
    "        key_points (list): Key points that should be in the answer\n",
    "        response (str): The generated response\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Count how many key points are mentioned in the response\n",
    "    mentioned_points = 0\n",
    "    for point in key_points:\n",
    "        if point.lower() in response.lower():\n",
    "            mentioned_points += 1\n",
    "    \n",
    "    coverage = mentioned_points / len(key_points) if key_points else 0\n",
    "    \n",
    "    return {\n",
    "        \"key_point_coverage\": coverage,\n",
    "        \"key_points_mentioned\": mentioned_points,\n",
    "        \"total_key_points\": len(key_points)\n",
    "    }\n",
    "\n",
    "# Run evaluation on our test cases\n",
    "print(\"Evaluating RAG performance...\")\n",
    "for case in evaluation_cases:\n",
    "    print(f\"\\nQuery: {case['query']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get RAG response\n",
    "    response = rag_query(case[\"query\"], vector_store, top_k=3)\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    retrieval_metrics = evaluate_retrieval(\n",
    "        case[\"query\"], case[\"relevant_docs\"], vector_store, top_k=3\n",
    "    )\n",
    "    \n",
    "    # Evaluate generation\n",
    "    generation_metrics = evaluate_generation(\n",
    "        case[\"query\"], case[\"key_points\"], response\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Retrieval Metrics:\")\n",
    "    print(f\"  Precision: {retrieval_metrics['precision']:.2f}\")\n",
    "    print(f\"  Recall: {retrieval_metrics['recall']:.2f}\")\n",
    "    print(f\"  F1 Score: {retrieval_metrics['f1']:.2f}\")\n",
    "    print(f\"  MRR: {retrieval_metrics['mrr']:.2f}\")\n",
    "    print(f\"  Retrieved: {retrieval_metrics['retrieved_titles']}\")\n",
    "    \n",
    "    print(\"\\nGeneration Metrics:\")\n",
    "    print(f\"  Key Point Coverage: {generation_metrics['key_point_coverage']:.2f}\")\n",
    "    print(f\"  Key Points Mentioned: {generation_metrics['key_points_mentioned']}/{generation_metrics['total_key_points']}\")\n",
    "    \n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d339ea",
   "metadata": {},
   "source": [
    "### Strategies for Improving RAG Performance\n",
    "\n",
    "Based on evaluation results, we can improve our RAG system in several ways:\n",
    "\n",
    "1. **Retrieval Improvements**:\n",
    "   - **Better chunking strategies**: Use semantic chunking instead of fixed-size chunking\n",
    "   - **Query reformulation**: Expand queries to better match relevant documents\n",
    "   - **Hybrid search**: Combine semantic search with keyword-based search\n",
    "   - **Re-ranking**: Use a second model to re-rank initial retrieval results\n",
    "\n",
    "2. **Generation Improvements**:\n",
    "   - **Prompt engineering**: Refine the prompt template for better use of retrieved context\n",
    "   - **Citation instruction**: Explicitly instruct the model to cite sources from the context\n",
    "   - **Filtering hallucinations**: Detect and remove generated content not supported by the context\n",
    "   \n",
    "Let's implement some of these improvements to our basic RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ed202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement some improvements to our RAG system\n",
    "\n",
    "# 1. Improvement: Query Reformulation\n",
    "def reformulate_query(original_query):\n",
    "    \"\"\"\n",
    "    Use the LLM to reformulate the query for better retrieval\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        \n",
    "    Returns:\n",
    "        str: The reformulated query\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Your task is to reformulate the given query to improve information retrieval.\n",
    "    Make the query more specific and include key terms that might appear in relevant documents.\n",
    "    Keep the reformulation focused and concise.\n",
    "    \n",
    "    Original Query: {original_query}\n",
    "    \n",
    "    Reformulated Query:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_gemini_response(prompt)\n",
    "    \n",
    "    # Clean up response to get just the reformulated query\n",
    "    # (In a production system, you'd want more robust parsing)\n",
    "    return response.strip()\n",
    "\n",
    "# 2. Improvement: Enhanced RAG with Query Reformulation and Better Prompting\n",
    "def enhanced_rag_query(user_query, vector_store, top_k=3, use_query_reformulation=True):\n",
    "    \"\"\"\n",
    "    Enhanced RAG query with improvements\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): User's question\n",
    "        vector_store: Vector store containing document chunks and embeddings\n",
    "        top_k (int): Number of documents to retrieve\n",
    "        use_query_reformulation (bool): Whether to use query reformulation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including response and metadata\n",
    "    \"\"\"\n",
    "    # 1. Reformulate the query if enabled\n",
    "    if use_query_reformulation:\n",
    "        retrieval_query = reformulate_query(user_query)\n",
    "        print(f\"Original query: {user_query}\")\n",
    "        print(f\"Reformulated query: {retrieval_query}\")\n",
    "    else:\n",
    "        retrieval_query = user_query\n",
    "    \n",
    "    # 2. Convert the query to an embedding\n",
    "    query_embedding = get_embedding(retrieval_query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    \n",
    "    if not query_embedding:\n",
    "        return {\"response\": \"Error: Could not generate embedding for query.\"}\n",
    "    \n",
    "    # 3. Retrieve relevant documents\n",
    "    search_results = vector_store.search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # 4. Format the retrieved context with clear source markers\n",
    "    context_chunks = []\n",
    "    for i, (item, score) in enumerate(search_results):\n",
    "        source_id = f\"[Source {i+1}]\"\n",
    "        context_chunks.append(f\"{source_id} Title: {item['chunk']['title']}\\nContent: {item['chunk']['content']}\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # 5. Create an improved prompt with clear instructions\n",
    "    improved_prompt = f\"\"\"\n",
    "    Answer the user's question based ONLY on the provided context. \n",
    "    \n",
    "    If the context doesn't contain enough information to provide a complete answer, acknowledge what you can answer \n",
    "    and clearly state what information is missing.\n",
    "    \n",
    "    IMPORTANT: \n",
    "    - Cite your sources using the [Source X] notation.\n",
    "    - Don't include information that isn't supported by the provided context.\n",
    "    - Be concise and focus on directly addressing the question.\n",
    "    \n",
    "    Context:\n",
    "    {context_text}\n",
    "    \n",
    "    User question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # 6. Generate response using the LLM\n",
    "    response = get_gemini_response(improved_prompt)\n",
    "    \n",
    "    # Return both the response and metadata for analysis\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"metadata\": {\n",
    "            \"original_query\": user_query,\n",
    "            \"retrieval_query\": retrieval_query,\n",
    "            \"retrieved_chunks\": [item[\"chunk\"][\"title\"] for item, _ in search_results],\n",
    "            \"similarity_scores\": [score for _, score in search_results]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Let's test our enhanced RAG system\n",
    "test_queries = [\n",
    "    \"How do RAG systems work?\",\n",
    "    \"What problems can RAG help solve?\",\n",
    "    \"What are the most recent developments in RAG technology?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nTesting enhanced RAG with query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get response using the enhanced RAG\n",
    "    result = enhanced_rag_query(query, vector_store, top_k=2)\n",
    "    \n",
    "    # Print metadata\n",
    "    print(\"\\nRetrieval Metadata:\")\n",
    "    for chunk_title, score in zip(result[\"metadata\"][\"retrieved_chunks\"], result[\"metadata\"][\"similarity_scores\"]):\n",
    "        print(f\"  - {chunk_title} (Score: {score:.4f})\")\n",
    "    \n",
    "    # Print response\n",
    "    print(\"\\nEnhanced RAG Response:\")\n",
    "    print(result[\"response\"])\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be0428",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG Techniques\n",
    "\n",
    "Beyond the basic RAG implementation we've explored, there are several advanced techniques that can significantly improve performance:\n",
    "\n",
    "### Multi-step RAG\n",
    "\n",
    "Multi-step RAG breaks the retrieval process into multiple stages:\n",
    "1. **Initial Query**: Start with the user's original query\n",
    "2. **Retrieval & Reading**: Retrieve documents and have the LLM read them\n",
    "3. **Query Refinement**: Generate a better query based on the initial findings\n",
    "4. **Final Retrieval**: Retrieve more accurate documents with the refined query\n",
    "5. **Response Generation**: Generate a comprehensive answer\n",
    "\n",
    "### Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "HyDE improves retrieval by:\n",
    "1. Having the LLM generate a hypothetical document that would answer the query\n",
    "2. Using this hypothetical document as the retrieval query instead of the original query\n",
    "3. This often leads to better semantic matching with relevant documents\n",
    "\n",
    "### Multi-Vector Retrieval\n",
    "\n",
    "Instead of representing a document with a single embedding vector:\n",
    "1. Split documents into multiple chunks\n",
    "2. Create an embedding for each chunk\n",
    "3. Retrieve based on chunk-level similarity\n",
    "4. This helps with long documents where different sections cover different topics\n",
    "\n",
    "Let's implement a simple version of multi-step RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1715a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement Multi-step RAG as an example of an advanced technique\n",
    "\n",
    "def multi_step_rag_query(user_query, vector_store, top_k_initial=3, top_k_final=2):\n",
    "    \"\"\"\n",
    "    Multi-step RAG query with query refinement\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): User's question\n",
    "        vector_store: Vector store containing document chunks and embeddings\n",
    "        top_k_initial (int): Number of documents for initial retrieval\n",
    "        top_k_final (int): Number of documents for final retrieval\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including response and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Original query: {user_query}\")\n",
    "    \n",
    "    # Step 1: Initial retrieval with original query\n",
    "    query_embedding = get_embedding(user_query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    initial_results = vector_store.search(query_embedding, top_k=top_k_initial)\n",
    "    \n",
    "    # Step 2: Format the initial context\n",
    "    initial_context = []\n",
    "    for item, score in initial_results:\n",
    "        initial_context.append(f\"Title: {item['chunk']['title']}\\nContent: {item['chunk']['content']}\")\n",
    "    \n",
    "    initial_context_text = \"\\n\\n\".join(initial_context)\n",
    "    \n",
    "    # Step 3: Have the LLM analyze the initial results and refine the query\n",
    "    refine_prompt = f\"\"\"\n",
    "    Based on the user's question and the initial search results, create a better search query.\n",
    "    The improved query should help retrieve more relevant information by including key terms found in the initial results.\n",
    "    \n",
    "    User's question: {user_query}\n",
    "    \n",
    "    Initial search results:\n",
    "    {initial_context_text}\n",
    "    \n",
    "    Provide ONLY the improved search query without any explanation or additional text:\n",
    "    \"\"\"\n",
    "    \n",
    "    improved_query = get_gemini_response(refine_prompt)\n",
    "    print(f\"Improved query: {improved_query}\")\n",
    "    \n",
    "    # Step 4: Second retrieval with improved query\n",
    "    improved_embedding = get_embedding(improved_query, task_type=\"RETRIEVAL_QUERY\")\n",
    "    final_results = vector_store.search(improved_embedding, top_k=top_k_final)\n",
    "    \n",
    "    # Step 5: Format the final context\n",
    "    final_context = []\n",
    "    for i, (item, score) in enumerate(final_results):\n",
    "        source_id = f\"[Source {i+1}]\"\n",
    "        final_context.append(f\"{source_id} Title: {item['chunk']['title']}\\nContent: {item['chunk']['content']}\")\n",
    "    \n",
    "    final_context_text = \"\\n\\n\".join(final_context)\n",
    "    \n",
    "    # Step 6: Generate the final answer\n",
    "    final_prompt = f\"\"\"\n",
    "    Answer the user's question based ONLY on the provided context. \n",
    "    \n",
    "    If the context doesn't contain enough information to provide a complete answer, acknowledge what you can answer \n",
    "    and clearly state what information is missing.\n",
    "    \n",
    "    IMPORTANT: \n",
    "    - Cite your sources using the [Source X] notation.\n",
    "    - Don't include information that isn't supported by the provided context.\n",
    "    - Be concise and focus on directly addressing the question.\n",
    "    \n",
    "    Context:\n",
    "    {final_context_text}\n",
    "    \n",
    "    User question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    final_response = get_gemini_response(final_prompt)\n",
    "    \n",
    "    # Return results and metadata\n",
    "    return {\n",
    "        \"response\": final_response,\n",
    "        \"metadata\": {\n",
    "            \"original_query\": user_query,\n",
    "            \"improved_query\": improved_query,\n",
    "            \"initial_results\": [item[\"chunk\"][\"title\"] for item, _ in initial_results],\n",
    "            \"final_results\": [item[\"chunk\"][\"title\"] for item, _ in final_results]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test the multi-step RAG\n",
    "complex_query = \"What techniques can help overcome the challenges of implementing effective RAG systems?\"\n",
    "\n",
    "print(\"\\nTesting Multi-step RAG:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "multi_step_result = multi_step_rag_query(complex_query, vector_store)\n",
    "\n",
    "# Print metadata\n",
    "print(\"\\nMulti-step RAG process:\")\n",
    "print(f\"1. Initial retrieval with: '{multi_step_result['metadata']['original_query']}'\")\n",
    "print(f\"   Retrieved: {', '.join(multi_step_result['metadata']['initial_results'])}\")\n",
    "print(f\"2. Query refinement: '{multi_step_result['metadata']['improved_query']}'\")\n",
    "print(f\"3. Final retrieval: {', '.join(multi_step_result['metadata']['final_results'])}\")\n",
    "\n",
    "# Print response\n",
    "print(\"\\nFinal Response:\")\n",
    "print(multi_step_result[\"response\"])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b92c918",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored Retrieval Augmented Generation (RAG) from the ground up:\n",
    "\n",
    "1. **Embeddings Fundamentals**: We learned how vector embeddings capture semantic relationships between words and documents, making them powerful tools for information retrieval.\n",
    "\n",
    "2. **Google's Embeddings API**: We utilized Google's text-embedding-004 model to create high-quality embeddings optimized for different tasks.\n",
    "\n",
    "3. **Basic RAG System**: We built a simple but functional RAG system that can retrieve relevant information and use it to generate accurate responses.\n",
    "\n",
    "4. **Evaluation and Fine-tuning**: We explored methods to evaluate RAG performance and implemented improvements like query reformulation and better prompting.\n",
    "\n",
    "5. **Advanced Techniques**: We implemented a multi-step RAG approach to handle more complex queries and improve retrieval accuracy.\n",
    "\n",
    "RAG represents one of the most practical ways to enhance LLMs for real-world applications. By grounding LLM outputs in retrieved facts, we can reduce hallucinations, provide access to specialized knowledge, and create more trustworthy AI systems.\n",
    "\n",
    "As you continue to work with RAG, remember that both the retrieval and generation components require ongoing refinement. The best RAG systems are those that are continuously evaluated and improved based on real-world usage and feedback.\n",
    "\n",
    "## Additional Resources and References\n",
    "\n",
    "Here are some valuable resources for further exploration of RAG:\n",
    "\n",
    "### Research Papers\n",
    "- Lewis, P., et al. (2020). \"[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\"\n",
    "- Guu, K., et al. (2020). \"[REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)\"\n",
    "- Borgeaud, S., et al. (2022). \"[Improving language models by retrieving from trillions of tokens](https://arxiv.org/abs/2112.04426)\"\n",
    "\n",
    "### Guides and Tutorials\n",
    "- [Google's Embedding API Documentation](https://ai.google.dev/api/embeddings)\n",
    "- [LangChain RAG Documentation](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [LlamaIndex RAG Guide](https://docs.llamaindex.ai/en/stable/use_cases/query_engine/)\n",
    "\n",
    "### Community Resources\n",
    "- [Hugging Face Embeddings Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "- [Awesome RAG GitHub Repository](https://github.com/Danielskry/Awesome-RAG)\n",
    "\n",
    "As RAG continues to evolve, staying up-to-date with the latest techniques will help you build increasingly powerful and accurate information retrieval and generation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tip-llm-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
